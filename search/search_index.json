{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ROS 2 Essentials","text":"<p>A repo containing essential ROS2 Humble features for controlling Autonomous Mobile Robots (AMRs) and robotic arm manipulators. Please setup an Ubuntu environment before using this repo.</p> <p>The goal of this repo is to allow seamless robot policy reuse between simulation and reality powered by (Omniverse) Isaac Sim, Isaac Lab, and Isaac ROS. In general, the amd64 images support both simulation and real robot control, while the arm64 images only supports real robot control.</p> <p>Please note that this repo is under rapid development. The code is not guaranteed to be stable, and breaking changes may occur.</p> <p>The documentation is hosted on https://j3soon.github.io/ros2-essentials/.</p>"},{"location":"#system-requirements","title":"System Requirements","text":"Use Case Platform Hardware Software Notes Simulation/Deployment x86_64 RTX GPU, 500GB+ SSD Ubuntu 22.04, NVIDIA Driver, Docker, NVIDIA Container Toolkit See this page for more details. Deployment-Only Jetson Jetson Orin, 500GB+ SSD JetPack 6 See this page for more details. <p>Make sure to install the required software prerequisites before using this repo.</p> <p>Some functionalities may still work on lower-spec systems, such as those without GPUs or on operating systems other than Ubuntu 22.04. However, these configurations are not officially supported and may require manual adjustments. Use them with caution.</p>"},{"location":"#setup","title":"Setup","text":"<pre><code>git clone https://github.com/j3soon/ros2-essentials.git\ncd ros2-essentials\n./scripts/post_install.sh\n</code></pre> <p>and then configure the container user ID to match your host user ID by modifying the host <code>~/.bashrc</code> (or <code>~/.zshrc</code>) to include the following line:</p> <pre><code>export USER_UID=$(id -u)\n</code></pre> <p>This step is optional if you have user ID 1000 on host.</p> <p>Next, choose a workspace from the table below and follow its documentation to get started. The rest of this document contains optional information.</p>"},{"location":"#pre-built-workspaces","title":"Pre-built Workspaces","text":"<p>Pre-built Docker images for each workspace can be pulled by running <code>docker compose pull</code> in the corresponding workspace directory. Pulling these images bypasses the time-consuming build process (for both Docker Compose and Dev Containers).</p> <p>The docker image of the template workspace is share by most of the workspace, allowing saving spaces by sharing common packages. Click on the following workspaces to navigate to their respective documentation.</p> Workspace amd64 arm64 Notes Maintainer Template \u2714\ufe0f \u2714\ufe0f Yu-Zhong Chen, Johnson Sun ORB-SLAM3 \u2714\ufe0f \u274c Assume Zhan RTAB-Map \u2714\ufe0f \u274c Assume Zhan ROS1 Bridge \u2714\ufe0f \u2714\ufe0f Skip linting Yu-Zhong Chen Cartographer \u2714\ufe0f \u2714\ufe0f Assume Zhan Clearpath Husky \u2714\ufe0f \u2714\ufe0f Real-world support Yu-Zhong Chen, Johnson Sun Yujin Robot Kobuki \u2714\ufe0f \u2714\ufe0f Real-world support Yu-Zhong Chen Velodyne VLP-16 \u2714\ufe0f \u2714\ufe0f Real-world support Assume Zhan Gazebo World \u2714\ufe0f \u274c\ufe0f Yu-Zhong Chen ALOHA \u2714\ufe0f \u2714\ufe0f Simulation only Johnson Sun Turtlebot3 \u2714\ufe0f \u274c\ufe0f Simulation only Johnson Sun Tesollo Delto Gripper \u2714\ufe0f \u274c\ufe0f Simulation only Yu-Zhong Chen Unitree Go2 \u2714\ufe0f \u274c\ufe0f Simulation only Yu-Zhong Chen, Assume Zhan, Johnson Sun Unitree H1 \u2714\ufe0f \u274c\ufe0f Simulation only Johnson Sun <p>If you have trouble using a workspace, please open an issue and tag the current maintainers mentioned above.</p>"},{"location":"#pre-configured-modules","title":"Pre-configured Modules","text":"Module Notes Default Maintainer ROS2 ROS2 Humble \u2714\ufe0f Yu-Zhong Chen CUDA Toolkit CUDA 12.6 \u274c Johnson Sun Isaac Sim Isaac Sim 4.5.0 Binary Install \u2714\ufe0f Johnson Sun Isaac Lab Isaac Lab 2.1.0 Git Install \u2714\ufe0f Johnson Sun Isaac ROS Isaac ROS 3.2 Apt Install (Base only) \u2714\ufe0f Johnson Sun"},{"location":"#building-documentation","title":"Building Documentation","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# Create and activate python virtual environment\nuv venv --python 3.10\nsource .venv/bin/activate\n# Install dependencies and start serving\ncd docs\nuv pip install -r requirements.txt\nmkdocs serve\n# Go to https://127.0.0.1:8000 to view the site.\n</code></pre>"},{"location":"#link-documentation-to-ros2-workspaces","title":"Link Documentation to ROS2 Workspaces","text":"<pre><code>scripts/setup_docs_link.sh\n</code></pre> <p>This is automatically done by running <code>./scripts/post_install.sh</code>.</p>"},{"location":"#vscode-intellisense","title":"VSCode Intellisense","text":"<p>If you have installed Isaac Sim 4.5.0 to the default path <code>~/isaacsim</code>, you can simply enable IntelliSense for editing any Isaac Sim scripts by running the following script:</p> <pre><code>scripts/setup_isaac_link.sh\n</code></pre> <p>This is automatically done by running <code>./scripts/post_install.sh</code>.</p>"},{"location":"#reusing-docker-build-cache","title":"Reusing Docker Build Cache","text":"<p>The default <code>docker</code> build driver cannot pull pre-built caches from Docker Hub. Using the <code>docker-container</code> driver enables cache reuse but may add a few minutes of overhead for sending tarballs after the build. You should consider this tradeoff if you choose to switch the build driver.</p>"},{"location":"#switching-to-docker-container-build-driver","title":"Switching to <code>docker-container</code> Build Driver","text":"<pre><code>docker buildx ls\n# Install buildx as `docker build` alias\ndocker buildx install\n# Create and use new builder\ndocker buildx create --name docker-container --driver docker-container --driver-opt default-load=true --use\n</code></pre> <p>After setting this, using <code>docker compose build</code> will use the build cache from Docker Hub. If you want to switch back to the default <code>docker</code> build driver, follow the instructions below.</p>"},{"location":"#switching-back-to-docker-build-driver","title":"Switching back to <code>docker</code> Build Driver","text":"<pre><code># Uninstall buildx as `docker build` alias\ndocker buildx uninstall\n# Unuse the created builder and remove it\ndocker buildx use default\ndocker buildx stop docker-container\ndocker buildx rm -f --all-inactive\n</code></pre>"},{"location":"#developer-notes","title":"Developer Notes","text":""},{"location":"#github-actions","title":"GitHub Actions","text":"<p>The GitHub Actions workflow is designed to share build caches between workspaces efficiently. The template workspace is built first, and its cache is then reused by other workspaces. This means that while the template workspace build appears in the commit history, other workspace builds are triggered indirectly and only show up in the GitHub Actions tab. For implementation details, see commit <code>024f52a</code>.</p> <p>Some current CI builds are flaky and may require re-running.</p>"},{"location":"#docker-compose-cleanup","title":"Docker Compose Cleanup","text":"<pre><code># cd into a workspace directory's docker directory\ndocker compose down --volumes --remove-orphans\ndocker volume rm ros2-gazebo-cache\ndocker volume rm ros2-isaac-sim-cache\n</code></pre>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>The code is mainly contributed by Johnson Sun, Yu-Zhong Chen, Assume Zhan, and others. For a full list of contributors, please refer to the contribution list.</p> <p>We extend our gratitude to ElsaLab and NVIDIA AI Technology Center (NVAITC) for their support in making this project possible.</p> <p>Disclaimer: this is not an official NVIDIA product.</p>"},{"location":"#license","title":"License","text":"<p>All modifications are licensed under Apache License 2.0.</p> <p>However, this repository includes many dependencies released under different licenses. For information on these licenses, please check the commit history. Make sure to review the license of each dependency before using this repository.</p> <p>The licenses for dependencies will be clearly documented in the workspace README in the future.</p>"},{"location":"#supplementary","title":"Supplementary","text":""},{"location":"#installing-docker","title":"Installing Docker","text":"<p>Follow this post for the installation instructions.</p>"},{"location":"#installing-nvidia-container-toolkit","title":"Installing NVIDIA Container Toolkit","text":"<p>Follow this post for the installation instructions.</p>"},{"location":"aloha-ws/","title":"ALOHA","text":""},{"location":"aloha-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/aloha_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-aloha-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"aloha-ws/#view-robot-model-in-rviz","title":"View Robot Model in RViz","text":"<pre><code>ros2 launch interbotix_xsarm_descriptions xsarm_description.launch.py robot_model:=vx300s use_joint_pub_gui:=true\n</code></pre> <p>It is worth noting that <code>aloha_vx300s.urdf.xacro</code> and <code>vx300s.urdf.xacro</code> files are identical. We opt to use <code>vx300s</code> since <code>aloha_vx300s</code> seems to lack corresponding configs, such as those for MoveIt 2.</p> <p></p>"},{"location":"aloha-ws/#view-robot-model-in-gazebo","title":"View Robot Model in Gazebo","text":"<pre><code>ros2 launch interbotix_xsarm_sim xsarm_gz_classic.launch.py robot_model:=vx300s\n</code></pre>"},{"location":"aloha-ws/#ros-2-control","title":"ROS 2 Control","text":"<pre><code>ros2 launch interbotix_xsarm_control xsarm_control.launch.py robot_model:=vx300s use_sim:=true\n# and then use the `Interbotix Control Panel`.\n</code></pre>"},{"location":"aloha-ws/#moveit-2-with-rviz","title":"MoveIt 2 with RViz","text":"<pre><code>ros2 launch interbotix_xsarm_moveit xsarm_moveit.launch.py robot_model:=vx300s hardware_type:=fake\n</code></pre>"},{"location":"aloha-ws/#moveit-2-with-gazebo","title":"MoveIt 2 with Gazebo","text":"<pre><code>ros2 launch interbotix_xsarm_moveit xsarm_moveit.launch.py robot_model:=vx300s hardware_type:=gz_classic\n</code></pre>"},{"location":"aloha-ws/#moveit-2-with-isaac-sim","title":"MoveIt 2 with Isaac Sim","text":"<p>Prepare USD files:</p> <pre><code>cd /home/ros2-essentials/aloha_ws/isaacsim/scripts\n./create_urdf_from_xacro.sh\npython3 create_vx300s_from_urdf.py\npython3 create_vx300s_with_omnigraph.py\n</code></pre> <p>and run:</p> <pre><code>ros2 launch interbotix_xsarm_moveit xsarm_moveit.launch.py robot_model:=vx300s hardware_type:=isaac\n# and then move the target and use the `MotionPlanning` panel.\n</code></pre> <p></p>"},{"location":"aloha-ws/#debugging-with-isaac-sim","title":"Debugging with Isaac Sim","text":"<p>The Isaac Sim app can be launched with:</p> <pre><code>~/isaacsim/isaac-sim.sh\n</code></pre> <p>Keep in mind that the standalone scripts can be easily debugged in Isaac Sim's <code>Script Editor</code>. Simply copy the code, omitting anything related to SimulationApp (remove the beginning and end), and paste to the <code>Script Editor</code> and run it.</p> <p>To open pre-configured USD file with OmniGraph:</p> <ul> <li><code>File &gt; Open</code> and click <code>My Computer</code>, then in <code>File name:</code> type:   <pre><code>/home/ros2-essentials/aloha_ws/isaacsim/assets/vx300s_og.usd\n</code></pre></li> <li>Click <code>Window &gt; Visual Scripting &gt; Action Graph</code></li> <li>In the <code>Action Graph</code> tab, click <code>Edit Action Graph</code> and select <code>/ActionGraph</code></li> <li>Click <code>Play (SPACE)</code></li> </ul> <p>View the current joint states:</p> <pre><code># in a new terminal\ndocker exec -it ros2-aloha-ws bash\nros2 topic echo /vx300s/joint_states\n</code></pre> <p>A specific world can also be directly launched and played with:</p> <pre><code>~/isaacsim/isaac-sim.sh --exec '/home/ros2-essentials/aloha_ws/isaacsim/scripts/open_isaacsim_stage.py --path /home/ros2-essentials/aloha_ws/isaacsim/assets/vx300s_og.usd'\n</code></pre> <p>To access Nucleus from Isaac Sim, you should install Nucleus with default username/password <code>admin:admin</code> on your host machine or connect to an external Nucleus server.</p>"},{"location":"aloha-ws/#references","title":"References","text":"<ul> <li>Interbotix X-Series Arms | Trossen Robotics Documentation<ul> <li>ROS 2 Interface<ul> <li>ROS 2 Standard Software Setup</li> </ul> </li> <li>ROS 2 Open Source Packages</li> </ul> </li> <li>Stationary ALOHA Software Setup | Trossen Robotics Documentation</li> </ul>"},{"location":"cartographer-ws/","title":"Cartographer","text":""},{"location":"cartographer-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/cartographer_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-cartographer-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"cartographer-ws/#simple-test-with-turtlebot3","title":"Simple Test With Turtlebot3","text":"<ul> <li> <p>Attach to the container</p> <p><pre><code>docker attach ros2-cartographer-ws\ncd /home/ros2-essentials/cartographer_ws\n</code></pre> - Open the turtlebot simulation in <code>tmux</code></p> <p><pre><code>export TURTLEBOT3_MODEL=burger\nros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n</code></pre> - Run the SLAM node in new window of <code>tmux</code></p> <p><pre><code>ros2 launch turtlebot3_cartographer cartographer.launch.py is_sim:=True\n</code></pre> - Run the control tool in new window of <code>tmux</code></p> <pre><code>rqt_robot_steering\n</code></pre> </li> </ul>"},{"location":"cartographer-ws/#building-packages","title":"Building Packages","text":"<pre><code>cd /home/ros2-essentials/cartographer_ws\nrosdep update\nrosdep install --from-paths src --ignore-src --rosdistro humble -y\ncolcon build\n</code></pre> <p>After the build process, make sure to source the <code>install/setup.bash</code> file. Otherwise, ROS2 will not locate the executable files. You can open a new terminal to accomplish this.</p>"},{"location":"cartographer-ws/#multi-lidar-single-robot-slam-test","title":"Multi LiDAR - Single Robot SLAM test","text":""},{"location":"cartographer-ws/#simulation","title":"Simulation","text":"<ul> <li>multi_lidar_desp package: Description of a robot with multiple LiDARs.</li> <li> <p>multi_lidar_gazebo package: Gazebo simulation of the robot with robot state publisher.</p> <pre><code>ros2 launch multi_lidar_gazebo multi_lidar_gazebo.launch.py\n</code></pre> </li> </ul>"},{"location":"cartographer-ws/#run-the-slam-node","title":"Run the SLAM node","text":"<ul> <li>Run the cartographer SLAM node in new window of <code>tmux</code> <pre><code>ros2 launch cartographer_demo cartographer_demo.launch.py is_sim:=True\n</code></pre></li> </ul> <ul> <li>Run the control tool in new window of <code>tmux</code> <pre><code>rqt_robot_steering\n</code></pre></li> </ul>"},{"location":"cartographer-ws/#references","title":"References","text":"<ul> <li>Cartographer Demo</li> </ul>"},{"location":"delto-gripper-ws/","title":"Tesollo Delto Gripper","text":"<p>This repository will help you configure the environment for Delto Gripper quickly.</p>"},{"location":"delto-gripper-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/delto_gripper_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-delto-gripper-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"delto-gripper-ws/#testing","title":"Testing","text":""},{"location":"delto-gripper-ws/#launch-the-isaacsim","title":"Launch the Isaacsim","text":"<pre><code>~/isaacsim/isaac-sim.sh\n</code></pre> <p>After the Isaacsim is launched, open the delto gripper usd file and play the simulation. It is located at: <code>delto_gripper_ws/src/DELTO_M_ROS2/dg_isaacsim/dg5f_right/dg5f_right.usd</code></p>"},{"location":"delto-gripper-ws/#launch-the-delto-gripper-demo","title":"Launch the Delto Gripper Demo","text":"<pre><code>ros2 launch dg5f_isaacsim dg5f_right_isaacsim.launch.py\n</code></pre>    Your browser does not support the video tag."},{"location":"delto-gripper-ws/#references","title":"References","text":"<p>Dependencies:</p> <ul> <li>Tesollo-Delto/DELTO_M_ROS2 (at commit 942c166) is licensed under the BSD-3-Clause license.</li> </ul>"},{"location":"docker-modules/","title":"Docker Modules","text":""},{"location":"docker-modules/#robot-operating-system-2-ros-2","title":"Robot Operating System 2 (ROS 2)","text":"<p>ROS 2 Humble Apt Install.</p>"},{"location":"docker-modules/#cuda-toolkit","title":"CUDA Toolkit","text":"<p>CUDA Toolkit 12.6 Deb Install.</p> <p>This is often unnecessary when only Python is used, as <code>pip install torch</code> typically installs the appropriate version of the CUDA Toolkit automatically.</p>"},{"location":"docker-modules/#isaac-sim","title":"Isaac Sim","text":"<p>Isaac Sim 4.5.0 Binary Install.</p> <p>Depends on:</p> <ul> <li>Vulkan Configuration</li> <li>Username</li> </ul> <p>Note that CUDA Toolkit is not required for Isaac Sim.</p> <p>Compatibility test:</p> <pre><code>cd ~/isaac-sim-comp-check\n./omni.isaac.sim.compatibility_check.sh\n</code></pre> <p>Quick test:</p> <pre><code>cd ~/isaacsim\n./python.sh standalone_examples/api/isaacsim.core.api/time_stepping.py\n# or\n./python.sh standalone_examples/api/isaacsim.core.api/simulation_callbacks.py\n</code></pre> <p>Launch GUI:</p> <pre><code>cd ~/isaacsim\n./isaac-sim.sh\n</code></pre> <p>On Host:</p> <p>Quick test using official Docker image:</p> <pre><code>scripts/docker_run_official_isaac_sim.sh\n</code></pre>"},{"location":"docker-modules/#importing-urdf","title":"Importing URDF","text":"<p><code>File &gt; Import</code> and select the URDF file.</p> <p>If your URDF file is in the Xacro format, you may need to convert it to the URDF format first:</p> <pre><code>XACRO_FILE=&lt;XACRO_FILE&gt;.xacro\nURDF_FILE=&lt;URDF_FILE&gt;.urdf\nxacro $XACRO_FILE &gt; $URDF_FILE\n</code></pre> <p>You can also check the URDF hierarchy:</p> <pre><code>check_urdf $URDF_FILE\n</code></pre> <p>Note: URDF files are often not self-contained and may reference additional resources within their ROS package. For successful import, consider downloading/moving the entire package (such as the <code>&lt;ROBOT_NAME&gt;_description</code> directory) along with the URDF file. Otherwise, the import will fail.</p>"},{"location":"docker-modules/#isaac-lab","title":"Isaac Lab","text":"<p>Isaac Lab 2.1.0 Git Install.</p> <p>Depends on:</p> <ul> <li>Vulkan Configuration</li> <li>Username</li> <li>Isaac Sim</li> </ul> <p>Note that CUDA Toolkit is not required for Isaac Lab.</p> <p>Quick test:</p> <pre><code>cd ~/IsaacLab\n./isaaclab.sh -p scripts/tutorials/00_sim/log_time.py --headless\n# View the logs and press Ctrl+C to stop\n</code></pre> <p>Train Cartpole:</p> <pre><code>cd ~/IsaacLab\n./isaaclab.sh -p scripts/reinforcement_learning/rl_games/train.py --task=Isaac-Cartpole-v0 --headless\n# or\n./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Cartpole-v0 --headless\n# or\n./isaaclab.sh -p scripts/reinforcement_learning/skrl/train.py --task=Isaac-Cartpole-v0 --headless\n</code></pre> <p>On Host:</p> <p>Quick test using official Docker image:</p> <pre><code>scripts/docker_run_official_isaac_lab.sh\n</code></pre>"},{"location":"docker-modules/#isaac-ros","title":"Isaac ROS","text":"<p>Note: This is a work in progress. Currently only the base Isaac ROS is installed. Running examples requires additional dependencies that are not yet included in this setup.</p> <p>Isaac ROS 3.2.</p> <p>Depends on:</p> <ul> <li>ROS 2 Humble</li> <li>CUDA Toolkit</li> <li>and more...</li> </ul>"},{"location":"gazebo-world-ws/","title":"Gazebo World","text":"<p>This repository contains several Gazebo worlds, which are valuable for testing robots or agents in both indoor and outdoor environments.</p>"},{"location":"gazebo-world-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/gazebo_world_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-gazebo-world-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"gazebo-world-ws/#structure","title":"\ud83c\udf31 Structure \ud83c\udf31","text":"<pre><code>ros2-essentials\n\u251c\u2500\u2500 gazebo_world_ws\n|   \u251c\u2500\u2500 .devcontainer\n|   \u251c\u2500\u2500 docker\n|   \u251c\u2500\u2500 figure\n|   \u251c\u2500\u2500 src\n|   |   \u251c\u2500\u2500 aws-robomaker-hospital-world\n|   |   \u251c\u2500\u2500 aws-robomaker-small-house-world\n|   |   \u251c\u2500\u2500 aws-robomaker-small-warehouse-world\n|   |   \u251c\u2500\u2500 citysim\n|   |   \u251c\u2500\u2500 clearpath_playpen\n|   |   \u251c\u2500\u2500 gazebo_launch\n|   |   \u2514\u2500\u2500 turtlebot3_gazebo\n|   \u251c\u2500\u2500 .gitignore\n|   \u2514\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"gazebo-world-ws/#how-to-use","title":"\ud83d\udea9 How to use \ud83d\udea9","text":"<p>Available target worlds: - aws_hospital - aws_small_house - aws_warehouse - citysim - clearpath_playpen - turtlebot3</p> <p>The turtlebot3 offers multiple worlds to choose from. For more information, you can refer to the launch file located at <code>turtlebot3.launch.py</code> in the <code>gazebo_launch</code> package.</p>"},{"location":"gazebo-world-ws/#use-in-gazebo_world_ws-container","title":"Use in <code>gazebo_world_ws</code> container","text":"<p>Normally, you wouldn\u2019t want to use it inside the <code>gazebo_world_ws</code> container, since this workspace doesn\u2019t include any robots by default. However, we still provide the Dockerfile for this workspace, you can use it if you have specific requirements.</p> <pre><code># Build the workspace\ncd /home/ros2-essentials/gazebo_world_ws\ncolcon build --symlink-install\nsource /home/ros2-essentials/gazebo_world_ws/install/setup.bash\n\n# Launch the world\n# Replace &lt;target world&gt; with the name of the world you wish to launch.\nros2 launch gazebo_launch &lt;target world&gt;.launch.py\n# or launch turtlebot3 worlds, such as:\nros2 launch gazebo_launch turtlebot3.launch.py gazebo_world:=turtlebot3_dqn_stage3.world\n</code></pre>"},{"location":"gazebo-world-ws/#use-in-other-containerworkspace","title":"Use in other container/workspace","text":""},{"location":"gazebo-world-ws/#1-compile-packages","title":"1. Compile packages","text":"<p>To use it in other containers, remember to compile <code>gazebo_world_ws</code> first. Generally, you can compile it using other containers directly, as the required dependencies for these packages should already be installed in all workspaces. You should only use the Docker environment provided by <code>gazebo_world_ws</code> if you encounter issues with compilation or path settings.</p> <pre><code># Build the workspace\ncd /home/ros2-essentials/gazebo_world_ws\ncolcon build --symlink-install\n</code></pre>"},{"location":"gazebo-world-ws/#2-source-the-local_setupbash","title":"2. Source the <code>local_setup.bash</code>","text":"<p>Add the following lines into <code>.bashrc</code> file.</p> <pre><code># Source gazebo_world_ws environment\nGAZEBO_WORLD_WS_DIR=\"${ROS2_WS}/../gazebo_world_ws\"\nif [ ! -d \"${GAZEBO_WORLD_WS_DIR}/install\" ]; then\n    echo \"gazebo_world_ws has not been built yet. Building workspace...\"\n    cd ${GAZEBO_WORLD_WS_DIR}\n    colcon build --symlink-install\n    cd -\n    echo \"gazebo_world_ws built successfully!\"\nfi\nsource ${GAZEBO_WORLD_WS_DIR}/install/local_setup.bash\n</code></pre>"},{"location":"gazebo-world-ws/#3-launch-gazebo-in-the-launch-file","title":"3. Launch gazebo in the launch file","text":"<p>Add the code into your launch file.</p> <p>Remember to replace the <code>&lt;target world&gt;</code> with the one you want.</p> <pre><code>from launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.substitutions import PathJoinSubstitution, LaunchConfiguration\nfrom launch_ros.substitutions import FindPackageShare\n\nARGUMENTS = [\n    DeclareLaunchArgument(\n        \"launch_gzclient\",\n        default_value=\"True\",\n        description=\"Launch gzclient, by default is True, which shows the gazebo GUI\",\n    ),\n]\n\n\ndef generate_launch_description():\n\n    ...\n\n    # Launch Gazebo\n    launch_gazebo = IncludeLaunchDescription(\n        PathJoinSubstitution(\n            [\n                FindPackageShare(\"gazebo_launch\"), \n                \"launch\",\n                \"&lt;target world&gt;.launch.py\",\n            ],\n        ),\n        launch_arguments={\n            \"launch_gzclient\": LaunchConfiguration(\"launch_gzclient\"),\n        }.items(),\n    )\n\n    ...\n\n    ld = LaunchDescription(ARGUMENTS)\n    ld.add_action(launch_gazebo)\n\n    ...\n\n    return ld\n</code></pre>"},{"location":"gazebo-world-ws/#snapshot","title":"\u2728 Snapshot \u2728","text":"World Snapshot aws_hospital aws_small_house aws_warehouse citysim turtlebot3_stage3 turtlebot3_house turtlebot3_world clearpath_playpen"},{"location":"gazebo-world-ws/#troubleshooting","title":"\ud83d\udd0d Troubleshooting \ud83d\udd0d","text":""},{"location":"gazebo-world-ws/#getting-stuck-when-launching-gazebo","title":"Getting stuck when launching Gazebo","text":"<p>The first time you launch a Gazebo world might take longer because Gazebo needs to download models from the cloud to your local machine. Please be patient while it downloads. If it takes too long, like more than an hour, you can check the <code>gzserver</code> logs in <code>~/.gazebo</code> to see where it\u2019s getting stuck. The most common issue is using a duplicate port, which prevents Gazebo from starting. You can use <code>lsof -i:11345</code> to identify which process is using the port and then use <code>kill -9</code> to terminate it.</p>"},{"location":"gazebo-world-ws/#unable-to-find-gazebo_launch","title":"Unable to find <code>gazebo_launch</code>","text":"<p>Please make sure you have sourced the <code>local_setup.bash</code> and compiled <code>gazebo_world_ws</code>. If you encounter a path issue, try removing the <code>install</code>, <code>build</code>, and <code>log</code> folders in <code>gazebo_world_ws</code> and compile the workspace in your container again.</p>"},{"location":"go2-ws/","title":"Unitree Go2","text":"<p>Please note that this workspace is only tested in simulation.</p>"},{"location":"go2-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/go2_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-go2-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"go2-ws/#testing","title":"Testing","text":""},{"location":"go2-ws/#isaac-lab-examples","title":"Isaac Lab Examples","text":"<p>Training environments (<code>Isaac-Velocity-Flat-Unitree-Go2-v0</code>, <code>Isaac-Velocity-Rough-Unitree-Go2-v0</code>):</p> <pre><code>cd ~/IsaacLab\n./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py --task Isaac-Velocity-Rough-Unitree-Go2-v0 --headless\n# or\n./isaaclab.sh -p scripts/reinforcement_learning/skrl/train.py --task Isaac-Velocity-Rough-Unitree-Go2-v0 --headless\n</code></pre> <p>Run pre-trained model inference:</p> <pre><code>cd ~/IsaacLab\n./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py --task Isaac-Velocity-Rough-Unitree-Go2-v0 --num_envs 32 --use_pretrained_checkpoint\n</code></pre>"},{"location":"go2-ws/#champ-controller-demo","title":"Champ Controller Demo","text":"<ol> <li>Launch the Go2 in the Isaac Sim.</li> </ol> <pre><code>ros2 launch go2_bringup go2_bringup.launch.py\n</code></pre> <ol> <li>Launch the Champ controller</li> </ol> <pre><code>ros2 launch champ_bringup go2.launch.py\n</code></pre> <p>On the first launch, the Go2 in Isaac Sim may initially stand on its rear legs, which can cause it to fall backwards or sideways during the next step. To fix this, simply stop and restart the Isaac Sim simulation (leave the CHAMP controller running), then proceed with the remaining steps.</p> <ol> <li>Send a command to the Go2</li> </ol> <p>We use the <code>teleop_twist_keyboard</code> for demonstration. You can use any other method as well.</p> <pre><code>ros2 run teleop_twist_keyboard teleop_twist_keyboard\n</code></pre>"},{"location":"go2-ws/#nav2-demo-with-champ-controller","title":"Nav2 Demo with Champ Controller","text":"<p>Note: This demo is only a quick verification that Nav2 can be used to control the Go2 to reach a target point. Many important steps, such as the Go2's odometry and SLAM, are omitted in this demonstration. In the current pipeline, Nav2 simply uses the ground-truth odometry data provided by Isaac Sim to control and move the Go2 to the target point.</p> <ol> <li>Launch the Go2 in the Isaac Sim.</li> </ol> <pre><code>ros2 launch go2_bringup go2_bringup.launch.py\n</code></pre> <ol> <li>Launch the Champ controller</li> </ol> <pre><code>ros2 launch champ_bringup go2.launch.py\n</code></pre> <ol> <li>Launch the Nav2</li> </ol> <pre><code>ros2 launch go2_navigation go2_navigation.launch.py\n</code></pre> <p>You can use the <code>2D Goal Pose</code> in RViz to set the target position, then Nav2 will plan a path and control the Go2 to reach it.</p> <p></p>"},{"location":"go2-ws/#custom-isaac-sim-environment","title":"Custom Isaac Sim Environment","text":"<p>Run <code>~/isaacsim/isaac-sim.sh</code> and open <code>/home/ros2-essentials/go2_ws/isaacsim/assets/go2_og.usda</code> in Omniverse, and then press Play.</p> <p> </p> <p>In another terminal, exec into the container:</p> <pre><code>docker exec -it ros2-go2-ws bash\n</code></pre> <p>Inspect the joint states and clock:</p> <pre><code>ros2 topic echo /joint_states\nros2 topic echo /clock\n</code></pre> <p>Inspect TF and Odom by launching <code>rviz2</code> and set <code>Fixed Frame</code> to <code>world</code> and <code>Add &gt; TF</code>. Then, <code>Add &gt; Odometry</code> and set <code>Topic</code> to <code>/odom</code>.</p> <p></p> <p>Send a joint command:</p> <pre><code>ros2 topic pub --once /joint_command sensor_msgs/msg/JointState \"{\n  name: [\n    'FL_hip_joint', 'FR_hip_joint', 'RL_hip_joint', 'RR_hip_joint',\n    'FL_thigh_joint', 'FR_thigh_joint', 'RL_thigh_joint', 'RR_thigh_joint',\n    'FL_calf_joint', 'FR_calf_joint', 'RL_calf_joint', 'RR_calf_joint'\n  ],\n  position: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n  velocity: [],\n  effort: []\n}\"\n</code></pre> <p>The Go2 should move forward a little bit, which can be seen in both Isaac Sim and RViz2.</p> <p> </p>"},{"location":"h1-ws/","title":"Unitree H1","text":"<p>Please note that this workspace is only tested in simulation.</p>"},{"location":"h1-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/h1_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-h1-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"h1-ws/#testing","title":"Testing","text":""},{"location":"h1-ws/#isaac-sim-examples","title":"Isaac Sim Examples","text":"<p>Deploying Policies:</p> <pre><code>cd ~/isaacsim\n./isaac-sim.sh\n</code></pre> <p><code>Window &gt; Examples &gt; Robotics Examples</code>, in the <code>Robotics Examples</code> window, click <code>POLICY &gt; Humanoid &gt; LOAD</code>.</p>"},{"location":"h1-ws/#isaac-lab-examples","title":"Isaac Lab Examples","text":"<p>Training environments (<code>Isaac-Velocity-Flat-H1-v0</code>, <code>Isaac-Velocity-Rough-H1-v0</code>):</p> <pre><code>cd ~/IsaacLab\n./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py --task Isaac-Velocity-Rough-H1-v0 --headless\n# or\n./isaaclab.sh -p scripts/reinforcement_learning/skrl/train.py --task Isaac-Velocity-Rough-H1-v0 --headless\n</code></pre> <p>Run pre-trained model inference:</p> <pre><code>cd ~/IsaacLab\n./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py --task Isaac-Velocity-Rough-H1-v0 --num_envs 32 --use_pretrained_checkpoint\n</code></pre> <p>Run H1 Locomotion Showroom Demo:</p> <pre><code>cd ~/IsaacLab\nexport PYTHONPATH=\"\"\n./isaaclab.sh -p scripts/demos/h1_locomotion.py\n</code></pre> <p>Clearing the PYTHONPATH is necessary to avoid the error:</p> <pre><code>[INFO] Using python from: /home/user/IsaacLab/_isaac_sim/python.sh\nTraceback (most recent call last):\n  File \"/home/user/IsaacLab/scripts/demos/h1_locomotion.py\", line 23, in &lt;module&gt;\n    import scripts.reinforcement_learning.rsl_rl.cli_args as cli_args  # isort: skip\n  File \"/opt/ros/humble/local/lib/python3.10/dist-packages/scripts/__init__.py\", line 15, in &lt;module&gt;\n    from .gazebo_ros_paths import GazeboRosPaths\n  File \"/opt/ros/humble/local/lib/python3.10/dist-packages/scripts/gazebo_ros_paths.py\", line 32, in &lt;module&gt;\n    from catkin_pkg.package import InvalidPackage, PACKAGE_MANIFEST_FILENAME, parse_package\nModuleNotFoundError: No module named 'catkin_pkg'\nThere was an error running python\n</code></pre>"},{"location":"h1-ws/#custom-isaac-sim-environment","title":"Custom Isaac Sim Environment","text":"<p>Run <code>~/isaacsim/isaac-sim.sh</code> and open <code>/home/ros2-essentials/h1_ws/isaacsim/assets/h1_og.usda</code> in Omniverse, and then press Play.</p> <p> </p> <p>In another terminal, exec into the container:</p> <pre><code>docker exec -it ros2-go2-ws bash\n</code></pre> <p>Inspect the joint states and clock:</p> <pre><code>ros2 topic echo /joint_states\nros2 topic echo /clock\n</code></pre> <p>Inspect TF and Odom by launching <code>rviz2</code> and set <code>Fixed Frame</code> to <code>world</code> and <code>Add &gt; TF</code>. Then, <code>Add &gt; Odometry</code> and set <code>Topic</code> to <code>/odom</code>.</p> <p></p> <p>Send a joint command:</p> <pre><code>ros2 topic pub --once /joint_command sensor_msgs/msg/JointState \"{\n  name: [\n    'left_hip_yaw_joint',\n    'right_hip_yaw_joint',\n    'torso_joint',\n    'left_hip_roll_joint',\n    'right_hip_roll_joint',\n    'left_shoulder_pitch_joint',\n    'right_shoulder_pitch_joint',\n    'left_hip_pitch_joint',\n    'right_hip_pitch_joint',\n    'left_shoulder_roll_joint',\n    'right_shoulder_roll_joint',\n    'left_knee_joint',\n    'right_knee_joint',\n    'left_shoulder_yaw_joint',\n    'right_shoulder_yaw_joint',\n    'left_ankle_joint',\n    'right_ankle_joint',\n    'left_elbow_joint',\n    'right_elbow_joint'\n  ],\n  position: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.57, -1.57, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  velocity: [],\n  effort: []\n}\"\n</code></pre> <p>The H1 should split its legs and fall to the ground, which can be seen in both Isaac Sim and RViz2.</p> <p> </p>"},{"location":"husky-ws/","title":"Clearpath Husky","text":"<p>This repository will help you configure the environment for Husky quickly.</p>"},{"location":"husky-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/husky_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-husky-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"husky-ws/#structure","title":"\ud83c\udf31 Structure \ud83c\udf31","text":"<p>Here is the structure of this workspace:</p> <pre><code>husky_ws\n\u251c\u2500\u2500 .devcontainer\n\u251c\u2500\u2500 docker\n\u251c\u2500\u2500 figure\n\u251c\u2500\u2500 install\n\u251c\u2500\u2500 build\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 script\n|   \u251c\u2500\u2500 husky-bringup.sh\n|   \u251c\u2500\u2500 husky-generate.sh\n|   \u2514\u2500\u2500 husky-teleop.sh\n\u251c\u2500\u2500 src\n|   \u251c\u2500\u2500 husky\n|   |   \u251c\u2500\u2500 husky_base\n|   |   \u251c\u2500\u2500 husky_bringup\n|   |   \u251c\u2500\u2500 husky_control\n|   |   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 udev_rules\n|   \u251c\u2500\u2500 41-clearpath.rules\n|   \u2514\u2500\u2500 install_udev_rules.sh\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre> <p><code>build</code> / <code>install</code> / <code>log</code> folders will appear once you've built the packages.</p>"},{"location":"husky-ws/#introduction","title":"\u2728 Introduction \u2728","text":"<p>This repository has been derived from the Clearpath Husky's repository. Here is the original repository. However, the original repository was designed for ROS1, and it is in the process of being upgraded to ROS2.</p> <p>Below are the main packages for Husky:</p> <ul> <li>husky_base : Base configuration</li> <li>husky_control : Control configuration</li> <li>husky_description : Robot description (URDF)</li> <li>husky_navigation : Navigation configuration</li> <li>husky_gazebo : Simulate environment</li> <li>husky_viz : Visualize data</li> </ul>"},{"location":"husky-ws/#testing","title":"\ud83d\udea9 Testing \ud83d\udea9","text":""},{"location":"husky-ws/#building-packages","title":"Building packages","text":"<p>Before attempting any examples, please remember to build the packages first. If you encounter any dependency errors, please use rosdep to resolve them.</p> <pre><code>cd /home/ros2-essentials/husky_ws\nrosdep update\nrosdep install --from-paths src --ignore-src --rosdistro humble -y\ncolcon build\n</code></pre> <p>After the build process, make sure to source the <code>install/setup.bash</code> file. Otherwise, ROS2 will not locate the executable files. You can open a new terminal to accomplish this.</p>"},{"location":"husky-ws/#view-the-model","title":"View the model","text":"<pre><code>ros2 launch husky_viz view_model_launch.py\n</code></pre>"},{"location":"husky-ws/#demonstration-of-slam","title":"Demonstration of SLAM.","text":"<pre><code>ros2 launch husky_navigation slam_launch.py\n</code></pre> <p>Rendering the model may take some time, so please be patient !</p>"},{"location":"husky-ws/#control-real-robot","title":"Control real robot","text":"<p>Before you proceed, please ensure that you've plugged the USB adapter of the Husky into the computer and mounted it into the container. (plugging in the USB adapter before creating the container is preferred but not required)</p> <pre><code># Move to the workspace, source .bashrc, and bringup husky.\ncd /home/ros2-essentials/husky_ws\nsource ~/.bashrc\n./script/husky-bringup.sh\n\n# (Optional) Open a new terminal &amp; control the robot via keyboard teleoperation.\n./script/husky-teleop.sh\n</code></pre>"},{"location":"husky-ws/#license","title":"License","text":"<p>To maintain reproducibility, we have frozen the following packages at specific commits. The licenses of these packages are listed below:</p> <ul> <li>husky/husky (at commit 1e0b1d1, <code>humble-devel</code> branch) is released under the BSD-3-Clause License.</li> <li>clearpathrobotics/LMS1xx (at commit 90001ac, <code>humble-devel</code> branch) is released under the LGPL License.</li> <li>osrf/citysim (at commit 3928b08) is released under the Apache-2.0 License.</li> <li>clearpathrobotics/clearpath_computer_installer (at commit 7e7f415) is released under the BSD-3-Clause License.</li> <li>clearpathrobotics/clearpath_robot/clearpath_robot/debian/udev (at commit 17d55f1) is released under the BSD 3-Clause License.</li> </ul> <p>Further changes based on the packages above are release under the Apache-2.0 License, as stated in the repository.</p>"},{"location":"kobuki-ws/","title":"Yujin Robot Kobuki","text":"<p>This repository facilitates the quick configuration of the simulation environment and real robot driver for Kobuki.</p>"},{"location":"kobuki-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/kobuki_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-kobuki-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"kobuki-ws/#introduction","title":"\u25fb\ufe0f Introduction \u25fb\ufe0f","text":"<p>This repository is primarily based on the kobuki-base. Below are the main packages for Kobuki:</p> Package Introduction kobuki_control The localization configuration of Kobuki kobuki_core Base configuration for Kobuki kobuki_gazebo Simulating Kobuki in Gazebo kobuki_launch Launch Kobuki in Gazebo or with a real robot kobuki_navigation SLAM setup for Kobuki kobuki_description Robot description (URDF) kobuki_node Kobuki Driver kobuki_rviz Visualizing data in RViz2 velodyne_simulator Simulating VLP-16 in Gazebo"},{"location":"kobuki-ws/#testing","title":"\ud83d\udea9 Testing \ud83d\udea9","text":""},{"location":"kobuki-ws/#building-packages","title":"Building packages","text":"<p>If you only need to bring up the real Kobuki robot, you don't need to compile the workspace. The Kobuki driver is already included in the Docker image. After the Docker image is built, you can directly bring up the robot.</p> <pre><code>cd /home/ros2-essentials/kobuki_ws\n\n# For x86_64 architecture\ncolcon build --symlink-install\n# For arm64 architecture\ncolcon build --symlink-install --packages-ignore velodyne_gazebo_plugins\n</code></pre> <ul> <li>The <code>--symlink-install</code> flag is optional, adding this flag may provide more convenience. See this post for more info.</li> <li>The <code>--packages-ignore</code> flag is used to ignore the <code>velodyne_gazebo_plugins</code> package. This package will use the <code>gazebo_ros_pkgs</code> package, which is not supported in the arm64 architecture.</li> <li>Please note that the building process for the embedded system may take a long time and could run out of memory. You can use the flag <code>--parallel-workers 1</code> to reduce the number of parallel workers, or you can build the packages on a more powerful machine and then transfer the executable files to the embedded system. For guidance on building the packages on a x86_64 architecture and then transferring the files to an arm64 architecture, refer to the <code>cross-compilation</code> section at the end of this README.</li> </ul> <p>After the build process, make sure to source the <code>install/setup.bash</code> file. Otherwise, ROS2 will not locate the executable files. You can open a new terminal to accomplish this.</p>"},{"location":"kobuki-ws/#visualize-the-model-in-rviz","title":"Visualize the model in RViz","text":"<pre><code>ros2 launch kobuki_rviz view_model_launch.py\n</code></pre> <p>You can view the published states under <code>TF &gt; Frames &gt; wheel_left_link</code> and <code>TF &gt; Frames &gt; wheel_right_link</code> in RViz.</p>"},{"location":"kobuki-ws/#launch-the-robot-in-gazebo","title":"Launch the robot in Gazebo","text":"<pre><code>ros2 launch kobuki_launch kobuki.launch.py is_sim:=true\n</code></pre>"},{"location":"kobuki-ws/#launch-the-robot-in-the-real-world","title":"Launch the robot in the real world","text":"<pre><code># Inside the container\ncd /home/ros2-essentials/kobuki_ws\n./script/kobuki-bringup.sh\n\n# or Outside the container\ncd /path/to/kobuki_ws/docker\ndocker compose run kobuki-ws /home/ros2-essentials/kobuki_ws/script/kobuki-bringup.sh\n</code></pre> <p>If you have successfully connected to the Kobuki, you should hear a sound from it. Otherwise, there may be errors. You can try re-plugging the USB cable, restarting the Kobuki, or even restarting the container.</p> <p>If you encounter an error message like the one below or a similar error in the terminal, please ensure that your USB cable is properly connected to the Kobuki and that the connection is stable. Additionally, if the Kobuki's battery is low, communication failures are more likely to occur. Please charge the Kobuki fully before trying again.</p> <p>To control the Kobuki with a keyboard, you can use the <code>teleop_twist_keyboard</code> package.</p> <pre><code># Recommend speed: \n# - Linear 0.1\n# - Angular 0.3\n\n# Inside the container\ncd /home/ros2-essentials/kobuki_ws\n./script/kobuki-teleop.sh\n\n# or Outside the container\ncd /path/to/kobuki_ws/docker\ndocker compose run kobuki-ws /home/ros2-essentials/kobuki_ws/script/kobuki-teleop.sh\n</code></pre>"},{"location":"kobuki-ws/#launch-the-demo-of-slam","title":"Launch the demo of SLAM","text":"<p>This demo is based on the slam_toolbox package and only supports the simulation environment.</p> <pre><code>ros2 launch kobuki_navigation slam.launch.py\n</code></pre>"},{"location":"kobuki-ws/#cross-compilation","title":"Cross-compilation","text":"<p>Since the embedded system may not have enough memory to build the packages, or it may take a long time to build them, you can build the packages on a x86_64 machine and then copy the executable files to the arm64 machine.</p> <p>First, you need to set up the Docker cross-compilation environment. See this repo and this website for more info.</p> <pre><code># Install the QEMU packages\nsudo apt-get install qemu binfmt-support qemu-user-static\n# This step will execute the registering scripts\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes --credential yes\n# Testing the emulation environment\ndocker run --rm -t arm64v8/ros:humble uname -m\n</code></pre> <p>Secondly, modify the target platform for the Docker image in <code>docker/compose.yaml</code> by uncommenting the two lines below.</p> <pre><code>platforms:\n    - \"linux/arm64\"\n</code></pre> <p>Next, navigate to the <code>docker</code> folder and use the command <code>docker compose build</code> to build the Docker image.</p> <p>Note that the arm64 architecture is emulated by the QEMU, so it may consume a lot of CPU and memory resources. You should not use the <code>devcontainer</code> when building the packages for the arm64 architecture on x86_64 architecture, Otherwise, you may encounter some problems such as running out of memory. If you really want to use <code>devcontainer</code>, remove all vscode extensions in the <code>.devcontainer/devcontainer.json</code> file first.</p> <p>When the building process ends, use <code>docker compose up -d</code> and attach to the container by running <code>docker attach ros2-kobuki-ws</code>. After that, we can start building the ROS packages. If you have built the packages for the x86_64 architecture before, remember to delete the <code>build</code>, <code>install</code>, and <code>log</code> folders.</p> <pre><code>cd /home/ros2-essentials/kobuki_ws\ncolcon build --symlink-install --packages-ignore velodyne_gazebo_plugins\n</code></pre> <p>Once everything is built, we can start copying the files to the target machine (arm64 architecture). Please modify the command below to match your settings, such as the IP address.</p> <pre><code># (On Host) \ncd /path/to/kobuki_ws\n# Save the Docker image to the file.\ndocker save j3soon/ros2-kobuki-ws | gzip &gt; kobuki_image.tar.gz\n# Copy the file to the target machine.\nscp -C kobuki_image.tar ubuntu@192.168.50.100:~/\n# Copy the kobuki_ws to the target machine. \n# If the workspace exists on the target machine, you can simply copy the `build` and `install` folders to it.\nrsync -aP kobuki_ws ubuntu@192.168.50.100:~/\n</code></pre> <pre><code># (On Remote)\ncd ~/\n# Decompress the file.\ngunzip kobuki_image.tar.gz\n# Load the Docker image from the file.\ndocker load &lt; kobuki_image.tar\n# Verify that the Docker image has loaded successfully.\ndocker images | grep ros2-kobuki-ws\n</code></pre> <p>If you have completed all the commands above without encountering any errors, you can proceed to launch the robot. Refer to the steps above for more information.</p>"},{"location":"orbslam3-ws/","title":"ORB-SLAM3","text":""},{"location":"orbslam3-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/orbslam3_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-orbslam3-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"orbslam3-ws/#simple-test-with-dataset","title":"Simple Test With Dataset","text":"<ul> <li>Attach to the container   <pre><code>docker attach ros2-orbslam3-ws\ncd /home/ros2-essentials/orbslam3_ws\n</code></pre></li> <li>Prepare data, only need to be done once<ul> <li>Download dataset (~1.2G)   <pre><code>wget -P . http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_medium/V1_02_medium.bag\n</code></pre></li> <li>ROS1 bag and ROS2 bag conversion reference <pre><code>sudo pip3 install rosbags\nrosbags-convert ./V1_02_medium.bag\n</code></pre></li> </ul> </li> <li>Play the bag file in <code>tmux</code> <pre><code>ros2 bag play V1_02_medium/V1_02_medium.db3 --remap /cam0/image_raw:=/camera\n</code></pre></li> <li>Run the ORB-SLAM3 in a new <code>tmux</code> window   <pre><code>source ~/test_ws/install/local_setup.bash\nros2 run orbslam3 mono ~/ORB_SLAM3/Vocabulary/ORBvoc.txt ~/ORB_SLAM3/Examples_old/Monocular/EuRoC.yaml false\n</code></pre>   2 windows will pop up, showing the results.</li> </ul>"},{"location":"orbslam3-ws/#reference-repo-or-issues","title":"Reference repo or issues","text":"<ul> <li>Solve build failure</li> <li>ORB-SLAM3</li> <li>SLAM2 and Foxy docker</li> <li>Error when using humble</li> </ul>"},{"location":"ros1-bridge-ws/","title":"ROS1 Bridge","text":"<p>This workspace is utilized to create a bridge between ROS1 and ROS2-humble.</p>"},{"location":"ros1-bridge-ws/#introduction","title":"\u25fb\ufe0f Introduction \u25fb\ufe0f","text":"<p><code>ros1_bridge</code> provides a network bridge that enables the exchange of messages between ROS 1 and ROS 2. You can locate the original repository here.</p> <p>Within this workspace, you'll find a Dockerfile specifically crafted to build both ros-humble and ros1_bridge from their source code. This necessity arises due to a version conflict between the <code>catkin-pkg-modules</code> available in the Ubuntu repository and the one in the ROS.</p> <p>The official explanation</p> <p>Assuming you are already familiar with the ROS network architecture. If not, I recommend reading the tutorial provided below first.  </p> <ul> <li>https://wiki.ros.org/Master</li> <li>https://docs.ros.org/en/humble/Concepts/Basic/About-Discovery.html</li> </ul>"},{"location":"ros1-bridge-ws/#structure","title":"\ud83c\udf31 Structure \ud83c\udf31","text":"<p>Here is the structure of this repo:</p> <pre><code>ros1_bridge_ws\n\u251c\u2500\u2500 .devcontainer\n|   \u2514\u2500\u2500 devcontainer.json\n\u251c\u2500\u2500 docker\n|   \u251c\u2500\u2500 .dockerignore\n|   \u251c\u2500\u2500 .env\n|   \u251c\u2500\u2500 compose.yaml\n|   \u251c\u2500\u2500 compose.debug.yaml\n|   \u251c\u2500\u2500 Dockerfile\n|   \u2514\u2500\u2500 start-bridge.sh\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"ros1-bridge-ws/#how-to-use","title":"\ud83d\udea9 How to use \ud83d\udea9","text":"<p>The docker compose includes two services: <code>ros1-bridge</code> and <code>ros1-bridge-build</code>. The <code>ros1-bridge</code> service is typically sufficient for regular use, while <code>ros1-bridge-build</code> is intended for debugging, as it retains all the necessary build tools in the docker image.</p> <p>If you are not debugging <code>ros1-bridge</code>, it is recommended to use the terminal rather than VScode-devcontainer. By default, the VScode-devcontainer uses the <code>ros1-bridge-build</code> service.</p>"},{"location":"ros1-bridge-ws/#1-build-the-docker-image","title":"1. Build the docker image","text":"<p>While building the image directly from the Dockerfile is possible, it may not be the most efficient choice. To save time, you can pull the image from Dockerhub instead of compiling it from the source.</p> <p>If you still prefer to build the image yourself, please follow the instructions below:</p> <ul> <li>VScode user<ul> <li>Open the workspace in VScode, press <code>F1</code>, and enter <code>&gt; Dev Containers: Rebuild Container</code>.</li> </ul> </li> <li>Terminal user<ul> <li>Open a terminal, change the directory to the docker folder, and type <code>docker compose build</code>.</li> </ul> </li> </ul> <p>Please note that the build process may take approximately 1 hour to complete, with potential delays depending on your computer's performance and network conditions.</p>"},{"location":"ros1-bridge-ws/#2-adjust-the-parameters-in-the-env-file","title":"2. Adjust the parameters in the <code>.env</code> file","text":"<p>We've placed all ROS-related parameters in the <code>.env</code> file. Please adjust these parameters according to your environment. By default, we set the <code>ROS1</code> master at <code>127.0.0.1</code>, and the <code>ROS2</code> domain ID to <code>0</code>. These settings should work for most scenarios. </p> <p>Please note that if these parameters are not configured correctly, the <code>ros1_bridge</code> will not function properly!</p>"},{"location":"ros1-bridge-ws/#3-start-the-container","title":"3. Start the container","text":"<ul> <li>VScode user<ul> <li>If you build the image through the devcontainer, it will automatically start the container.   After you get into the container, type <code>./start-bridge.sh</code> in the terminal.</li> </ul> </li> <li>Terminal user<ul> <li>Open a terminal, change the directory to the docker folder, and type <code>docker compose up</code>.</li> </ul> </li> </ul>"},{"location":"ros1-bridge-ws/#4-launch-rosmaster-in-ros1","title":"4. Launch rosmaster in ROS1","text":"<p>This step is automatically executed when you run <code>docker compose up</code>.</p> <p>As mentioned in https://github.com/ros2/ros1_bridge/issues/391, you should avoid using <code>roscore</code> in ROS1 to prevent the issue of not bridging <code>/rosout</code>. Instead, use <code>rosmaster --core</code> as an alternative.</p>"},{"location":"ros1-bridge-ws/#5-begin-communication","title":"5. Begin communication","text":"<p>You have successfully executed all the instructions. Now, you can proceed to initiate communication between ROS1 and ROS2-humble.</p> <p>Please keep in mind that the bridge will be established only when there are matching publisher-subscriber pairs active for a topic on either side of the bridge.</p>"},{"location":"ros1-bridge-ws/#example","title":"\u2728 Example \u2728","text":""},{"location":"ros1-bridge-ws/#run-the-bridge-and-the-example-talker-and-listener","title":"Run the bridge and the example talker and listener","text":"<p>Before beginning the example, ensure you have four containers ready:</p> <ul> <li><code>ros-core</code></li> <li><code>ros2-ros1-bridge-ws</code></li> <li><code>ros1</code></li> <li><code>ros2</code></li> </ul> <p>When using <code>ros1-bridge</code> in your application scenarios, you only need the <code>ros-core</code> and <code>ros2-ros1-bridge-ws</code> containers. Please replace the <code>ros1</code> and <code>ros2</code> containers with your application containers, as those are only included for demonstration purposes and are not required for using <code>ros1-bridge</code>.  </p> <p>Furthermore, ensure that you mount <code>/dev/shm</code> into both the <code>ros2-ros1-bridge-ws</code> and <code>ros2</code> containers, and that all containers share the host network.</p>"},{"location":"ros1-bridge-ws/#1-start-the-ros1_bridge-and-other-container","title":"1. Start the <code>ros1_bridge</code> and other container","text":"<pre><code># In docker folder\ndocker compose up\n</code></pre> <p>This command will start the four containers mentioned above.</p>"},{"location":"ros1-bridge-ws/#2-run-the-talker-and-listener-node","title":"2. Run the talker and listener node","text":"<p>We run the listener node in the <code>ros1</code> container and the talker node in the <code>ros2</code> container. You can run the talker node in <code>ros1</code> and the listener node in <code>ros2</code> if you'd like. To achieve this, modify the command provided below.</p>"},{"location":"ros1-bridge-ws/#ros1","title":"ROS1","text":"<pre><code>docker exec -it ros1 /ros_entrypoint.sh bash\n# Inside ros1 container\nrosrun roscpp_tutorials listener\n# or\n# rosrun roscpp_tutorials talker\n</code></pre>"},{"location":"ros1-bridge-ws/#ros2","title":"ROS2","text":"<pre><code>docker exec -it ros2 /ros_entrypoint.sh bash\n# Inside ros2 container\n# Use the same UID as ros1_bridge to prevent Fast-DDS shared memory permission issues.\n# Ref: https://github.com/j3soon/ros2-essentials/pull/9#issuecomment-1795743063\nuseradd -ms /bin/bash user\nsu user\nsource /opt/ros/humble/setup.bash\nros2 run demo_nodes_cpp talker\n# or\n# ros2 run demo_nodes_cpp listener\n</code></pre> <p>Certainly, you can try the example provided by <code>ros1_bridge</code>. However, there's no need to source the setup script within the <code>ros2-ros1-bridge-ws</code> container, simply starting the container will suffice.</p>"},{"location":"ros1-bridge-ws/#troubleshooting","title":"\ud83d\udd0d Troubleshooting \ud83d\udd0d","text":"<p>If you are trying to debug <code>ros1_bridge</code>, it is recommended to use the <code>ros1-bridge-build</code> service in docker compose. It contains all the necessary build tools, which should be helpful for you.</p>"},{"location":"ros1-bridge-ws/#failed-to-contact-ros-master","title":"Failed to contact ros master","text":"<p>Before launching <code>ros-core</code>, make sure to adjust the <code>ROS_MASTER_URI</code> correctly. For more information, please check the <code>.env</code> file and this section.</p> <p>You can replace <code>127.0.0.1</code> with the actual IP address or hostname of your ros master. This configuration ensures that your ros nodes know where to find the ros master for communication. Remember, in addition to modifying the parameters for <code>ros1_bridge</code>, you also need to adjust the parameters for your own container!</p>"},{"location":"ros1-bridge-ws/#ros2-cant-receive-the-topic","title":"ROS2 can't receive the topic","text":"<p>The latest releases of Fast-DDS come with the shared memory transport enabled by default. Therefore, you need to mount shared memory, also known as <code>/dev/shm</code>, into every container you intend to communicate with when using Fast-DDS. This ensures proper communication between containers. Ensure that you use the same UID as <code>ros2-ros1-bridge-ws</code> to avoid Fast-DDS shared memory permission issues.</p> <p>Reference: https://github.com/eProsima/Fast-DDS/issues/1698#issuecomment-778039676</p>"},{"location":"rtabmap-ws/","title":"RTAB-Map","text":""},{"location":"rtabmap-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/rtabmap_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-rtabmap-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"rtabmap-ws/#lidar-test-with-gazebo","title":"LiDAR test with gazebo","text":"<ul> <li>Launch Gazebo with turtlebot3 in <code>tmux</code> <pre><code>ros2 launch rtabmap_sim sim_robot.launch.py\n</code></pre></li> <li>Run rtabmap LiDAR demo in a new <code>tmux</code> window   <pre><code>ros2 launch rtabmap_demos turtlebot3_scan.launch.py\n</code></pre></li> </ul>"},{"location":"rtabmap-ws/#rgbd-test-with-gazebo","title":"RGBD test with gazebo","text":"<ul> <li>Launch Gazebo with turtlebot3 in <code>tmux</code> <pre><code>ros2 launch rtabmap_sim sim_robot.launch.py\n</code></pre></li> <li>Run rtabmap LiDAR demo in a new <code>tmux</code> window   <pre><code>ros2 launch rtabmap_demos turtlebot3_rgbd.launch.py\n</code></pre></li> </ul>"},{"location":"rtabmap-ws/#dual-sensor-test-with-gazebo","title":"Dual sensor test with gazebo","text":"<ul> <li>Launch Gazebo with turtlebot3 in <code>tmux</code> <pre><code>ros2 launch rtabmap_sim sim_robot.launch.py\n</code></pre></li> <li>Run dual sensor demo in a new <code>tmux</code> window   <pre><code>ros2 launch rtabmap_sim dual_sensor.launch.py\n</code></pre></li> </ul>"},{"location":"rtabmap-ws/#run-with-rqt","title":"Run with rqt","text":"<ul> <li>Running in a new <code>tmux</code> window   <pre><code>rqt_robot_steering\n</code></pre></li> </ul>"},{"location":"rtabmap-ws/#result","title":"Result","text":"<ul> <li>After you've run the demo, you could find the following result directly.</li> </ul> <ol> <li>LiDAR test</li> </ol> <ol> <li>RGBD test</li> </ol> <ol> <li>Dual sensor test</li> </ol>"},{"location":"rtabmap-ws/#reference","title":"Reference","text":"<ul> <li>RTAB-Map wiki</li> </ul>"},{"location":"rtabmap-ws/#existing-issues","title":"Existing issues","text":"<ul> <li><code>VTK</code> warning   <pre><code>QVTKOpenGLWidget: Warning: In /build/vtk6-6.3.0+dfsg1/Rendering/OpenGL2/vtkOpenGLRenderWindow.cxx, line 781\n...\n</code></pre><ul> <li>It seems that the warning isn't a big deal. But it will interrupt debugging in the future.</li> <li>Possible solution : set <code>VTK_LEGACY_REMOVE</code>, but it required to build from source.     - Still not tested yet.</li> <li>Issue Reference</li> </ul> </li> </ul>"},{"location":"template-ws/","title":"Template","text":"<p>This template will help you set up a ROS-Humble environment quickly.</p>"},{"location":"template-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/template_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-template-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"template-ws/#structure","title":"\ud83c\udf31 Structure \ud83c\udf31","text":"<p>Here is the structure of this template:</p> <pre><code>ros2-essentials\n\u251c\u2500\u2500 scripts\n|   \u2514\u2500\u2500 create_workspace.sh\n\u251c\u2500\u2500 template_ws\n|   \u251c\u2500\u2500 .devcontainer\n|   |   \u2514\u2500\u2500 devcontainer.json\n|   \u251c\u2500\u2500 docker\n|   |   \u251c\u2500\u2500 .bashrc\n|   |   \u251c\u2500\u2500 .dockerignore\n|   |   \u251c\u2500\u2500 compose.yaml\n|   |   \u2514\u2500\u2500 Dockerfile\n|   \u251c\u2500\u2500 install\n|   \u251c\u2500\u2500 build\n|   \u251c\u2500\u2500 log\n|   \u251c\u2500\u2500 src\n|   |   \u251c\u2500\u2500 minimal_pkg\n|   |   |   \u251c\u2500\u2500 include\n|   |   |   \u251c\u2500\u2500 minimal_pkg\n|   |   |   \u251c\u2500\u2500 scripts\n|   |   |   \u2514\u2500\u2500 ...\n|   |   \u2514\u2500\u2500 ...\n|   \u2514\u2500\u2500 README.md\n</code></pre> <ul> <li><code>build</code> / <code>install</code> / <code>log</code> folders will appear once you've built the packages.</li> </ul>"},{"location":"template-ws/#how-to-use-this-template","title":"\ud83d\udea9 How to use this template \ud83d\udea9","text":""},{"location":"template-ws/#1-use-the-script-to-copy-the-template-workspace","title":"1. Use the script to copy the template workspace.","text":"<p>We have provided a script to create a new workspace. Please use it to avoid potential issues.  </p> <pre><code># Open a terminal, and change the directory to ros2-essentials.\n./scripts/create_workspace.sh &lt;new_workspace_name&gt;\n</code></pre> <p>To unify the naming style, we will modify the string <code>&lt;new_workspace_name&gt;</code> in some files.</p>"},{"location":"template-ws/#2-configure-settings","title":"2. Configure settings.","text":"<p>To help you easily find where changes are needed, we have marked most of the areas you need to adjust with <code># TODO:</code>. Usually, you only need to modify the configurations without removing anything. If you really need to remove something, make sure you clearly understand your goal and proceed with caution.</p> <ul> <li><code>docker/Dockerfile</code>     - Add the packages you need according to the comments inside.</li> <li><code>docker/compose.yaml</code>     - By default, the Docker image is built according to your current computer's architecture. If you need to cross-compile, please modify the <code>platforms</code> parameter to your desired architecture and set up the basic environment.     - If you want to access the GPU in the container, please uncomment the lines accordingly.     - If you want to add any environment variables in the container, you can include them in the <code>environment</code> section, or you can use <code>export VARIABLE=/the/value</code> in <code>docker/.bashrc</code>.</li> <li><code>docker/.bashrc</code>     - We will automatically compile the workspace in .bashrc. If you don't want this to happen, feel free to remove it. If you\u2019re okay with it, remember to adjust the compilation commands according to your packages.</li> <li><code>src</code>     - Add the ros packages you need here.     - <code>minimal_pkg</code> is the ROS2 package used to create a publisher and subscriber in both Python and C++. You can remove it if you don't need it.</li> </ul>"},{"location":"template-ws/#3-open-the-workspace-folder-using-visual-studio-code","title":"3. Open the workspace folder using Visual Studio Code.","text":"<p>Haven't set up the devcontainer yet ?</p> <p>Please refer to the tutorial provided by Visual Studio Code first. You can find it here:  https://code.visualstudio.com/docs/devcontainers/containers</p> <p>We recommend using <code>VScode</code> + <code>devcontainer</code> for development. This plugin can significantly reduce development costs and can be used on local computers, remote servers, and even embedded systems. If you don't want to use <code>devcontainer</code>, you can still use Docker commands for development, such as <code>docker compose up</code> and <code>docker exec</code>.</p> <p>Open the workspace folder using Visual Studio Code, spotting the workspace folder within your Explorer indicates that you've selected the wrong folder. You should only observe the <code>.devcontainer</code>, <code>docker</code> and <code>src</code> folders there.</p>"},{"location":"template-ws/#4-build-the-container","title":"4. Build the container.","text":"<p>We have pre-built some Docker images on Docker Hub. If the building time is too long, you might consider downloading them from Docker Hub instead. For more information, please refer to the <code>README.md</code> on the repository's main page.</p> <p>Users are encouraged to set this argument in their shell configuration (e.g., <code>.bashrc</code> or <code>.zshrc</code>) if using UID different from <code>1000</code>:</p> <pre><code>export USER_UID=$(id -u)\n</code></pre> <p>Press <code>F1</code> and enter <code>&gt; Dev Containers: Rebuild Container</code>. Building the images and container will take some time. Please be patient.</p> <p>You should see the output below.</p> <pre><code>Done. Press any key to close the terminal.\n</code></pre> <p>For non-devcontainer users, please navigate to the <code>docker</code> folder and use <code>docker compose build</code> to build the container. We have moved all commands that need to be executed into the <code>.bashrc</code> file. No further action is needed after creating the Docker container.</p>"},{"location":"template-ws/#5-start-to-develop-with-ros","title":"5. Start to develop with ROS.","text":"<p>You've successfully completed all the instructions. Wishing you a productive and successful journey in your ROS development !</p>"},{"location":"template-ws/#warning","title":"\u26a0\ufe0f Warning \u26a0\ufe0f","text":"<ul> <li>Do not place your files in any folder named <code>build</code>, <code>install</code>, or <code>log</code>. These folders will not be tracked by Git.</li> <li>If you encounter an error when opening Gazebo, consider closing the container and reopen it. Alternatively, you can check the log output in <code>~/.gazebo</code>, which may contain relevant error messages. The most common issue is using a duplicate port, which prevents Gazebo from starting. You can use <code>lsof -i:11345</code> to identify which process is using the port and then use <code>kill -9</code> to terminate it.</li> <li><code>xhost +local:docker</code> is required if the container is not in privileged mode.</li> </ul>"},{"location":"turtlebot3-ws/","title":"TurtleBot3","text":"<p>Please note that this workspace is only tested in simulation.</p>"},{"location":"turtlebot3-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/turtlebot3_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-turtlebot3-ws bash\n</code></pre> <p>TurtleBot3 <code>burger</code> is used by default, you can change it by the following (such as <code>waffle</code> or <code>waffle_pi</code>):</p> <pre><code>export TURTLEBOT3_MODEL=waffle_pi\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"turtlebot3-ws/#inspect-robot-model","title":"\ud83d\udd0d Inspect Robot Model","text":"<p>Skip this section if you are not interested in inspecting the URDF/USD model.</p> <p>(TODO)</p>"},{"location":"turtlebot3-ws/#launch-simulator","title":"\ud83e\ude90 Launch Simulator","text":"<p>Launch one of the following simulators (1) RViz Fake Node (2) Gazebo (3) Isaac Sim.</p>"},{"location":"turtlebot3-ws/#rviz-fake-node","title":"RViz Fake Node","text":"<p>Note that RViz fake node doesn't simulate sensors such as LiDAR.</p> <pre><code>ros2 launch turtlebot3_fake_node turtlebot3_fake_node.launch.py\n</code></pre>"},{"location":"turtlebot3-ws/#gazebo","title":"Gazebo","text":"<p>Launch one of the following worlds:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\nros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\nros2 launch turtlebot3_gazebo turtlebot3_house.launch.py\n</code></pre>"},{"location":"turtlebot3-ws/#isaac-sim","title":"Isaac Sim","text":"<p>Convert URDF file to USD file and generate OmniGraph:</p> <pre><code>cd /home/ros2-essentials/turtlebot3_ws/isaacsim/scripts\n./create_urdf_from_xacro.sh\npython3 create_turtlebot3_burger_from_urdf.py\npython3 create_turtlebot3_burger_with_omnigraph.py\n</code></pre> <p>Start Isaac Sim in GUI mode:</p> <pre><code>~/isaacsim/isaac-sim.sh\n</code></pre> <p>Alternatively, start Isaac Sim in headless WebRTC mode:</p> <pre><code>isaac-sim.streaming.sh\n</code></pre> <p>and use the WebRTC Streaming Client.</p> <p>Open the file with OmniGraph we just generated in the bottom panel:</p> <pre><code>/home/ros2-essentials/turtlebot3_ws/isaacsim/assets/turtlebot3_burger_og.usd\n</code></pre> <p>or use <code>File &gt; Open Recent</code> to re-open it, and click <code>Play (SPACE)</code> to start simulation.</p> <p>To view the OmniGraph, right click the ActionGraph on the right panel and select <code>Open Graph</code>.</p>"},{"location":"turtlebot3-ws/#features","title":"\ud83e\udde9 Features","text":"<p>Skip this section if you are not interested in enabling features.</p> <p>Note that Cartographer and Nav2 features are only supported in Gazebo simulation for now.</p>"},{"location":"turtlebot3-ws/#cartographer","title":"Cartographer","text":"<p>Basic LiDAR SLAM.</p> <pre><code>ros2 launch turtlebot3_cartographer cartographer.launch.py use_sim_time:=True\n</code></pre>"},{"location":"turtlebot3-ws/#nav2","title":"Nav2","text":"<p>Requires a running SLAM or a pre-generated map.</p> <pre><code>ros2 launch turtlebot3_navigation2 navigation2.launch.py use_sim_time:=true map:=$ROS2_WS/map.yaml\n</code></pre>"},{"location":"turtlebot3-ws/#control-robot","title":"\ud83d\udd79\ufe0f Control Robot","text":""},{"location":"turtlebot3-ws/#raw-message","title":"Raw Message","text":"<p>Move forward:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/Twist \"{'linear': {'x': 0.2, 'y': 0.0, 'z': 0.0}, 'angular': {'x': 0.0, 'y': 0.0, 'z': 0.0}}\"\n</code></pre> <p>Stop:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/Twist \"{'linear': {'x': 0.0, 'y': 0.0, 'z': 0.0}, 'angular': {'x': 0.0, 'y': 0.0, 'z': 0.0}}\"\n</code></pre>"},{"location":"turtlebot3-ws/#teleoperate","title":"Teleoperate","text":"<p>Teleoperate through keyboard:</p> <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre>"},{"location":"turtlebot3-ws/#simple-auto-drive","title":"Simple Auto Drive","text":"<p>Simple collision avoidance:</p> <pre><code>ros2 run turtlebot3_gazebo turtlebot3_drive\n</code></pre>"},{"location":"turtlebot3-ws/#visualize-with-rviz","title":"Visualize with RViz","text":"<p>Open a new terminal and visualize published topics in RViz:</p> <pre><code>ros2 launch turtlebot3_bringup rviz2.launch.py\n</code></pre>"},{"location":"turtlebot3-ws/#save-output","title":"\ud83d\udcbe Save Output","text":"<p>Skip this section if you are not interested in saving maps.</p>"},{"location":"turtlebot3-ws/#occupancy-grid-map","title":"Occupancy Grid Map","text":"<pre><code>ros2 run nav2_map_server map_saver_cli -f $ROS2_WS/map\n</code></pre>"},{"location":"turtlebot3-ws/#references","title":"\ud83d\udccc References","text":"<ul> <li>TurtleBot3 | ROBOTIS Documentation</li> </ul>"},{"location":"vlp-ws/","title":"Velodyne VLP-16","text":""},{"location":"vlp-ws/#start-container","title":"\ud83d\udc33 Start Container","text":"<p>Make sure your system meets the system requirements and have followed the setup instructions before using this workspace.</p> <p>Run the following commands in a Ubuntu desktop environment. If you are using a remote server, make sure you're using a terminal within a remote desktop session (e.g., VNC) instead of SSH (i.e., don't use <code>ssh -X</code> or <code>ssh -Y</code>).</p> <pre><code>cd ~/ros2-essentials/vlp_ws/docker\ndocker compose pull\ndocker compose build\nxhost +local:docker\ndocker compose up -d\n# The initial build will take a while, please wait patiently.\n</code></pre> <p>If your user's UID is <code>1000</code>, you may skip the <code>docker compose build</code> command.</p> <p>The commands in the following sections assume that you are inside the Docker container:</p> <pre><code># in a new terminal\ndocker exec -it ros2-vlp-ws bash\n</code></pre> <p>If the initial build somehow failed, run:</p> <pre><code>rm -r build install\ncolcon build --symlink-install\n</code></pre> <p>Once you have finished testing, you can stop and remove the container with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"vlp-ws/#simulation-setup","title":"Simulation Setup","text":""},{"location":"vlp-ws/#add-description-in-defined-robot","title":"Add description in defined robot","text":"<ol> <li> <p>Declared necessary argument and  your own <code>robot_gazebo.urdf.xacro</code></p> <pre><code>&lt;xacro:arg name=\"gpu\" default=\"false\"/&gt;\n&lt;xacro:arg name=\"organize_cloud\" default=\"false\"/&gt;\n&lt;xacro:property name=\"gpu\" value=\"$(arg gpu)\" /&gt;\n&lt;xacro:property name=\"organize_cloud\" value=\"$(arg organize_cloud)\" /&gt;\n</code></pre> </li> <li> <p>Include the <code>velodyne_description</code> package</p> <pre><code>&lt;xacro:include filename=\"$(find velodyne_description)/urdf/VLP16.urdf.xacro\" /&gt;\n</code></pre> </li> <li> <p>Add LiDAR in the robot</p> <pre><code>&lt;xacro:VLP-16 parent=\"base_footprint\" name=\"velodyne\" topic=\"/velodyne_points\" organize_cloud=\"${organize_cloud}\" hz=\"10\" samples=\"440\" gpu=\"${gpu}\"&gt;\n    &lt;origin xyz=\"0 0 0.1\" rpy=\"0 0 0\" /&gt;\n&lt;/xacro:VLP-16&gt;\n</code></pre> </li> </ol> <ul> <li>You could refer to more information from <code>veloyne_simulator/velodyne_description/urdf/template.urdf.xacro</code></li> </ul>"},{"location":"vlp-ws/#launch-lidar-driver-with-simulated-lidar","title":"Launch LiDAR driver with simulated LiDAR","text":"<ul> <li>Launch example robot with LiDAR<pre><code>ros2 launch velodyne_description example.launch.py\n</code></pre> </li> </ul> <ul> <li>Launch the LiDAR data process without driver in another terminal<pre><code>ros2 launch vlp_cartographer vlp_driver.launch.py is_sim:=True\n</code></pre> </li> </ul>"},{"location":"vlp-ws/#sample-robot","title":"Sample Robot","text":"<ul> <li>A given turtlebot waffle model with VLP-16 is provided in <code>velodyne_simulator/velodyne_simulator/urdf/sample_robot.urdf.xacro</code></li> <li>Launch the sample robot<pre><code>ros2 launch velodyne_description example.launch.py robot:=1\n</code></pre> </li> </ul>"},{"location":"vlp-ws/#lidar-setup","title":"LiDAR setup","text":""},{"location":"vlp-ws/#hardware-setup","title":"Hardware Setup","text":"<ul> <li>Connect the LiDAR to power.</li> <li>Connect the LiDAR to the computer or router using the provided ethernet cable.</li> </ul>"},{"location":"vlp-ws/#directly-using-computer","title":"Directly Using Computer","text":"<ul> <li>Connect the LiDAR to the computer using the ethernet cable.</li> <li>Open the computer settings and navigate to Network &gt; Wired.</li> </ul> <ul> <li>Set the IPv4 configuration to 'manual' and configure the settings as shown in the image below:</li> </ul>"},{"location":"vlp-ws/#launch-the-driver","title":"Launch the Driver","text":""},{"location":"vlp-ws/#pipeline","title":"Pipeline","text":"<ul> <li>Data process as following:   raw data -&gt; pointcloud -&gt; laser scan -&gt; slam method</li> </ul> <ul> <li>Velodyne driver: <code>velodyne_driver</code> get the raw data from LiDAR.</li> <li>Transform the raw data to pointcloud: <code>velodyne_pointcloud</code></li> <li>Transform the pointcloud to laser scan: <code>velodyne_laserscan</code></li> </ul>"},{"location":"vlp-ws/#operating-in-a-single-launch-file","title":"Operating in a single launch file","text":"<pre><code>ros2 launch vlp_cartographer vlp_driver.launch.py\n</code></pre> <ul> <li>By the above command, the driver, pointcloud and laserscan will be launched.</li> </ul>"},{"location":"vlp-ws/#published-topics","title":"Published topics","text":"Topic Type Description <code>/velodyne_packets</code> <code>velodyne_msgs/VelodyneScan</code> raw data <code>/velodyne_points</code> <code>sensor_msgs/PointCloud2</code> Point cloud message <code>/scan</code> <code>sensor_msgs/LaserScan</code> laser scan message"},{"location":"vlp-ws/#test-with-cartographer","title":"Test with cartographer","text":"<ul> <li>In another terminal, launch the cartographer node:<pre><code>ros2 launch vlp_cartographer cartographer_demo.launch.py\n</code></pre> </li> </ul>"},{"location":"vlp-ws/#bringup","title":"Bringup","text":"<ul> <li>Build the docker image and workspace<pre><code>docker compose up --build\n</code></pre> </li> </ul> <ul> <li>LiDAR driver<pre><code>docker exec -it ros2-vlp-ws /home/ros2-essentials/vlp_ws/scripts/lidar-driver-bringup.sh\n</code></pre> </li> </ul> <ul> <li>After launching the driver, launch the cartographer in another terminal<pre><code>docker exec -it ros2-vlp-ws /home/ros2-essentials/vlp_ws/scripts/lidar-slam-bringup.sh\n</code></pre> </li> </ul>"},{"location":"vlp-ws/#reference","title":"Reference","text":"<ul> <li>Velodyne_driver with ROS2 Humble</li> <li>Cartographer ROS</li> <li>Turtlebot3 with Cartographer</li> </ul>"}]}